{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c707f35e",
   "metadata": {},
   "source": [
    "# Exam Planning-Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81dac69",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233976dd",
   "metadata": {},
   "source": [
    "Consider the following environment:\n",
    "\n",
    "<img src=\"images/env1.png\" alt=\"ex1\" style=\"zoom: 40%;\" />\n",
    "\n",
    "The agent starts in cell $(0, 0)$ and has to reach the treasure in $(2, 3)$. In addition to the walls of the previous environments, the floor is covered with lava, however the agent can resist to high temperature and it can use the heat to recharge its batteries, hence it receives a positive reward for being on a lava cell. The environment is deterministic and the agent must avoid the black pits of death (cells $(0,3)$, $(1, 3)$, $(1,1)$). \n",
    "Assume that you do not have access to the motion model and to reward and that the problem is undiscounted (i.e., $\\gamma$=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5be1e677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['S' 'L' 'L' 'P']\n",
      " ['L' 'P' 'L' 'P']\n",
      " ['L' 'L' 'L' 'G']]\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "module_path = os.path.abspath(os.path.join('tools'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import gym, envs\n",
    "from utils.ai_lab_functions import *\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm as tqdm\n",
    "import numpy as np\n",
    "\n",
    "def epsilon_greedy(q, state, epsilon):\n",
    "    \"\"\"\n",
    "    Epsilon-greedy action selection function\n",
    "    \n",
    "    Args:\n",
    "        q: q table\n",
    "        state: agent's current state\n",
    "        epsilon: epsilon parameter\n",
    "    \n",
    "    Returns:\n",
    "        action id\n",
    "    \"\"\"\n",
    "    an = q.shape[1]\n",
    "    probs = np.full(an, epsilon / an)\n",
    "    probs[q[state].argmax()] += 1 - epsilon\n",
    "    return np.random.choice(an, p=probs)\n",
    "\n",
    "\n",
    "env = gym.make('LavaFloor-v0')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc195b2",
   "metadata": {},
   "source": [
    "### 1.1) Given the environment reported above, find a policy with a suitable algorithm of your choice. Print the resulting policy (you can use the provided code for this)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15e2e846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# Magari usiamo q-learning\n",
    "def q_learning(environment, episodes, alpha, gamma, expl_func, expl_param):\n",
    "    \"\"\"\n",
    "    Performs the Q-Learning algorithm for a specific environment\n",
    "    \n",
    "    Args:\n",
    "        environment: OpenAI Gym environment\n",
    "        episodes: number of episodes for training\n",
    "        alpha: alpha parameter\n",
    "        gamma: gamma parameter\n",
    "        expl_func: exploration function (epsilon_greedy, softmax)\n",
    "        expl_param: exploration parameter (epsilon, T)\n",
    "    \n",
    "    Returns:\n",
    "        (policy, rewards, lengths): final policy, rewards for each episode [array], length of each episode [array]\n",
    "    \"\"\"\n",
    "    \n",
    "    q = np.zeros((environment.observation_space.n, environment.action_space.n))  # Q(s, a)\n",
    "    rews = np.zeros(episodes) # rewawrds per episode \n",
    "    lengths = np.zeros(episodes)\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            a = expl_func(q,state,expl_param)\n",
    "            next_state, reward, done, _ = env.step(a)  # Execute a step, if done== True it has reached the Goal \n",
    "            \n",
    "            # Useless stuff\n",
    "            rews[episode] += reward\n",
    "            lengths[episode]+= 1\n",
    "            \n",
    "            val = [0 for _ in environment.actions]\n",
    "            for action in environment.actions: \n",
    "                val[action]= q[next_state,action]\n",
    "            q[state,a] = q[state,a] + alpha*(reward + gamma* max(val) - q[state,a])\n",
    "            if done: break # magari va appena dopo il calolo del next state ? NO stai calcolando lo stato prima e non lo stato del goal !  ! ! \n",
    "            state = next_state\n",
    "    \n",
    "    policy = q.argmax(axis=1) # q.argmax(axis=1) automatically extract the policy from the q table\n",
    "    return policy, rews, lengths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ce36292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy:\n",
      " [['D' 'L' 'L' 'L']\n",
      " ['D' 'L' 'U' 'L']\n",
      " ['R' 'R' 'R' 'L']] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "episodes = 5000 # For giving it enough time to converge, bot not too much\n",
    "# Why 200 episodes ? ? ? \n",
    "gamma = .9 # peso dell'azione migliore nello next_state appena scelto basasto sulla q ancora da aggiornare \n",
    "alpha = .3  # quanto vale la differenza che aggiorna il q, effettivamente quanto VELOCEMENTE IMPARIAMO \n",
    "epsilon = .1 # epsilon per epsilon greedy \n",
    "\n",
    "env = gym.make('LavaFloor-v0')\n",
    "policy, rewards, lengths = q_learning(env,episodes, alpha, gamma, epsilon_greedy, epsilon)\n",
    "print(\"Policy:\\n {} \\n\".format(np.vectorize(env.actions.get)(policy.reshape(env.shape))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72ae25a",
   "metadata": {},
   "source": [
    "### 1.2) Find a way to force the agent to choose the shortest path towards the goal. You can change these parameters: episodes, alpha, gamma exploration function and exploration parameter. \n",
    "\n",
    "##### Hint: you should tune a parameter to consider the immediate reward rather than the long-term one... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86059fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy:\n",
      " [['D' 'L' 'L' 'L']\n",
      " ['D' 'L' 'U' 'L']\n",
      " ['R' 'R' 'R' 'L']] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "episodes = 500 \n",
    "alpha = .5 \n",
    "gamma = .1  # Quanto vale la value function della iterazione precedente \n",
    "epsilon = .1 # we want it to explore more  in epsilon greedy, the lower the epsilon is the more likely it will NOT take a random move to explore \n",
    "\n",
    "# Solution === high number of episodes and a small epsilon, small epsilon can be attributed to the small state_size of the environment. \n",
    "\n",
    "# Q-Learning or SARSA epsilon greedy\n",
    "env = gym.make('LavaFloor-v0')\n",
    "p, r, l =  q_learning(env,episodes, alpha, gamma, epsilon_greedy, epsilon)\n",
    "print(\"Policy:\\n {} \\n\".format(np.vectorize(env.actions.get)(p.reshape(env.shape))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbff4e4",
   "metadata": {},
   "source": [
    "### 1.3) Consider and environment with states {A, B, C, D}, actions {r, l} where states {A, D} are terminal. Consider the following sequence of learning episodes:\n",
    "* E1: (B, r, C, −1),(C, r, C, −1),(C, r, D, +1)\n",
    "* E2: (B, r, C, −1),(C, r, D, +1)\n",
    "* E3: (B, l, A, +5)\n",
    "* E4: (B, l, B, −1),(B, l, B, −1),(B, l, A, +5)\n",
    "### Compute v(s) for all non-terminal states by using a sample-based evaluation approach (i.e., computing the values with the function reported below). Assume $\\alpha$ = .5 and $\\gamma$ = 1.\n",
    "### $V(s) = (1- \\alpha) \\cdot V(s) + \\alpha \\cdot (r + \\gamma \\cdot V'(s))$ \n",
    "### where $s$ is the state under consideration (first element of each tuple), $s'$  (third element of each tuple ) the state reached by starting from $s$ and performing the action $a$ (second element of each tuple). And finally, $r$ is the reward (last element of each tuple)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "017a0627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 5, 'B': 6.90625, 'C': 1.375, 'D': 1}\n"
     ]
    }
   ],
   "source": [
    "episodes = {1: [('B', 'r', 'C', -1), ('C', 'r', 'C', -1),('C', 'r', 'D', 1)], \n",
    "            2: [('B', 'r', 'C', -1), ('C', 'r', 'D', 1)], \n",
    "            3: [('B', 'l', 'A', 5)], \n",
    "            4: [('B', 'l', 'B', -1),('B', 'l', 'B', -1),('B', 'l', 'A', 5)]}\n",
    "v = {'A': 5, 'B': 0, 'C': 0, 'D': 1}\n",
    "alpha = 0.5\n",
    "gamma = 1\n",
    "\n",
    "\n",
    "for episode, values in episodes.items():\n",
    "    for tuple in values:\n",
    "        v_pos,v1_pos,reward = tuple[0],tuple[2],tuple[3]\n",
    "        v[v_pos] = ((1-alpha)*v[v_pos]) + alpha*(reward + gamma* v[v1_pos])\n",
    "\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfc8f7c",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc5f593",
   "metadata": {},
   "source": [
    "Consider the figure below where **S=$(1,2)$** and **G=$(3,1)$** are the starting and goal positions respectively. Consider the problem of finding a minimum cost path from S to G assuming the agent can move in the four directions (if there is no\n",
    "obstacle) and that each movement has a unitary cost. The environment is deterministic. Answer the following questions:\n",
    "\n",
    "<img src=\"images/ex2.png\" alt=\"ex2\" style=\"zoom: 40%;\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed0ba51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['C' 'C' 'C' 'C' 'C']\n",
      " ['C' 'C' 'S' 'W' 'C']\n",
      " ['C' 'W' 'W' 'W' 'C']\n",
      " ['C' 'G' 'C' 'C' 'C']]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('SmallMaze-v0')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7062b99",
   "metadata": {},
   "source": [
    "### 2.1) Verify by using the code developed in the lab that the Manhattan distance (l1_norm) is a  *consistent* heuristic in this environment. In particular, you should implement a function that checks whether for every couple of state $(s,s')$ (where $s'$ is a successor state of $s$), the consistency condition is verified. The function should return true if the heuristic is consistent and false otherwise. Recall that every action has a cost of 1...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0352c701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# every action has the cost of 1 => breadth first search ? no dobbiamo usare un euristica, quindi la cosa più sensata è value iteration immagino essendo un mdp \n",
    "\n",
    "# def wrong_consistency_controll(state,state_1,heuristic_function,action_cost=1):\n",
    "#     # Consistency: h(n) <= cost + h(n')     --->     h(n) - h(n') <= cost\n",
    "#     if (heuristic(state) - heuref consistency_controll(state,state_1,heuristic_function,action_cost=1):\n",
    "#     # Consistency: h(n) <= cost + h(n')     --->     h(n) - h(n') <= cost\n",
    "#     if (heuristic(state) - heuristic(state_1)) <= cost: return True\n",
    "#     else: return False\n",
    "    \n",
    "    \n",
    "def consistency_control(environment):\n",
    "    goalpos = environment.state_to_pos(environment.goalState)\n",
    "\n",
    "    for s in range(environment.observation_space.n)\n",
    "        if environment.grid[s] == \"W\": continue \n",
    "        for action in environment.actions:\n",
    "\n",
    "            s1 = environment.sample(s,action)\n",
    "            hn_s = Heu.l1_norm(environment.state_to_pos(s), goalpos)\n",
    "            hn_s1 = Heu.l1_norm(environment.state_to_pos(s1), goalpos)\n",
    "\n",
    "            if hn_s - hn1 <= 1: # action_cost = 1 \n",
    "                print(\"Not Consistent\")\n",
    "                return False \n",
    "\n",
    "    print(\"Consistent\")\n",
    "    return True \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97399378",
   "metadata": {},
   "source": [
    "### 2.2) Consider the A* algorithm and assume want to achieve optimality. Based on the *consistent* heuristics of Section 2.1, state whether it is best to use a graph search or tree search strategy. Motivate your answer and show the results of A* execution and the differences between the two versions in terms of the computed solution, number of nodes generated and maximum number of nodes in memory. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217679b5",
   "metadata": {},
   "source": [
    "Since we have a consistent heuristic we have to choose a graph search version to achieve optimality. We know only if tree search + admissible heuristic or graph search + consistent heuristic => optimality of A*. In all other situations, we cannot guarantee optimality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c74ec4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def present_with_higher_cost(queue, node):\n",
    "    if node.state in queue:\n",
    "        if queue[node.state].value > node.value: \n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9e5fcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def astar_tree_search(environment):\n",
    "    \"\"\"\n",
    "    A* Tree search\n",
    "    \n",
    "    Args:\n",
    "        problem: OpenAI Gym environment\n",
    "        \n",
    "    Returns:\n",
    "        (path, time_cost, space_cost): solution as a path and stats.\n",
    "    \"\"\"\n",
    "\n",
    "    goalpos = environment.state_to_pos(environment.goalstate)\n",
    "    queue = PriorityQueue()\n",
    "    queue.add(Node(environment.startstate, value=Heu.l1_norm(environment.state_to_pos(environment.startstate), goalpos)))\n",
    "    time_cost = 1\n",
    "    space_cost = 1\n",
    "    \n",
    "    while True:        \n",
    "        if queue.is_empty(): \n",
    "            return None, time_cost, space_cost\n",
    "    \n",
    "        node = queue.remove()\n",
    "        if node.state == environment.goalstate: \n",
    "            return build_path(node), time_cost, space_cost\n",
    "        \n",
    "        for action in range(environment.action_space.n): #Look around\n",
    "            child_state = environment.sample(node.state, action)\n",
    "            child = Node( \n",
    "                          child_state, # node state\n",
    "                          node, # node parent\n",
    "                          node.pathcost + 1, # incremental path cost\n",
    "                          node.pathcost + 1 + Heu.l1_norm(environment.state_to_pos(child_state), goalpos) # node value (heuristic)\n",
    "                         ) \n",
    "            \n",
    "            time_cost += 1\n",
    "            queue.add(child)\n",
    "            \n",
    "        space_cost = max(space_cost, len(queue))\n",
    "\n",
    "def astar_graph_search(environment):\n",
    "    \"\"\"\n",
    "    A* Graph Search\n",
    "    \n",
    "    Args:\n",
    "        problem: OpenAI Gym environment\n",
    "        \n",
    "    Returns:\n",
    "        (path, time_cost, space_cost): solution as a path and stats.\n",
    "    \"\"\"\n",
    "    \n",
    "    goalpos = environment.state_to_pos(environment.goalstate)\n",
    "    queue = PriorityQueue()\n",
    "    queue.add(Node(environment.startstate, value=Heu.l1_norm(environment.state_to_pos(environment.startstate), goalpos)))\n",
    "    time_cost = 1\n",
    "    space_cost = 1\n",
    "    explored = set()\n",
    "    \n",
    "    while True:\n",
    "        if queue.is_empty(): return None, time_cost, space_cost\n",
    "        \n",
    "        node = queue.remove()\n",
    "        if node.state == environment.goalstate: \n",
    "            return build_path(node), time_cost, space_cost\n",
    "        explored.add(node.state)\n",
    "            \n",
    "        for action in range(environment.action_space.n):  #Look around\n",
    "            child_state = environment.sample(node.state, action)\n",
    "            child = Node( \n",
    "                      child_state, # node state\n",
    "                      node, # node parent\n",
    "                      node.pathcost + 1, # incremental path cost\n",
    "                      node.pathcost + 1 + Heu.l1_norm(environment.state_to_pos(child_state), goalpos) # node value (heuristic)\n",
    "                     ) \n",
    "            time_cost += 1\n",
    "            \n",
    "            if child.state not in queue and child.state not in explored:\n",
    "                #if child.state == environment.goalstate: return build_path(node), time_cost, space_cost\n",
    "                queue.add(child)\n",
    "            elif present_with_higher_cost(queue, child):\n",
    "                queue.replace(child)\n",
    "                \n",
    "        space_cost = max(space_cost, len(queue) + len(explored))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "425883ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution (tree-search): [(1, 1), (1, 0), (2, 0), (3, 0), (3, 1)]\n",
      "N° of nodes explored: 125\n",
      "Max n° of nodes in memory: 94\n",
      "\n",
      "=================================================================\n",
      "\n",
      "Solution (graph-search): [(1, 1), (1, 0), (2, 0), (3, 0), (3, 1)]\n",
      "N° of nodes explored: 29\n",
      "Max n° of nodes in memory: 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('SmallMaze-v0')\n",
    "solution_ts, time_ts, memory_ts = astar_tree_search(env)\n",
    "solution_gs, time_gs, memory_gs = astar_graph_search(env)\n",
    "\n",
    "print(\"Solution (tree-search): {}\".format(solution_2_string(solution_ts, env)))\n",
    "print(\"N° of nodes explored: {}\".format(time_ts))\n",
    "print(\"Max n° of nodes in memory: {}\\n\".format(memory_ts))\n",
    "print('='*65)\n",
    "print(\"\\nSolution (graph-search): {}\".format(solution_2_string(solution_gs, env)))\n",
    "print(\"N° of nodes explored: {}\".format(time_gs))\n",
    "print(\"Max n° of nodes in memory: {}\\n\".format(memory_gs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f177b11c",
   "metadata": {},
   "source": [
    "### 2.3) Let us consider BFS, show the path of the optimal solution (avoiding the repetition of states) to achieve the goal. In this scenario can we guarantee that the returned solution is the least cost one? If we used Iterative Deepening Search(IDS) with the same methodology used for BFS can we guarantee a least cost solution? Justify your answer and show the results of the different strategies by using the code developed during the lab. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48591674",
   "metadata": {},
   "source": [
    "Given a unitary cost problem both bfs and Iterative Deepening Search guarantee to be optimal ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee43dcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your BFS code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c11d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SmallMaze-v0')\n",
    "solution_gs, time_gs, memory_gs = BFS_GraphSearch(env)\n",
    "print(\"Solution (BFS graph-search): {}\".format(solution_2_string(solution_gs, env)))\n",
    "print(\"N° of nodes explored: {}\".format(time_gs))\n",
    "print(\"Max n° of nodes in memory: {}\\n\".format(memory_gs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bfe7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example IDS+GRAPH SEARCH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41388738",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SmallMaze-v0')\n",
    "solution_gs, time_gs, memory_gs, iterations_gs = IDS(env, Recursive_DLS_GraphSearch)\n",
    "print(\"Solution (IDS graph-search): {}\".format(solution_2_string(solution_gs, env)))\n",
    "print(\"N° of nodes explored: {}\".format(time_gs))\n",
    "print(\"Max n° of nodes in memory: {}\\n\".format(memory_gs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89f593b",
   "metadata": {},
   "source": [
    "### Discuss the results achieved with respect to the 2.3 question:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe0e580",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9bc2c64",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de20f428",
   "metadata": {},
   "source": [
    "Consider the environment displayed in Figure below, where states $(0, 3)$ and $(1, 3)$ are terminal states with reward $+1$ and $−1$ respectively. The agent can move in the four directions. The transition model states that for every state and action the agent has $0.8$ chances of moving in the chosen direction and $0.1$ chances to move in the othogonal directions.The reward model states that for every state, action and successor state the agent pays $−0.04$. Assume that the dicount factor is set to $0.9$. Answer the following questions: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b78047",
   "metadata": {},
   "source": [
    "<img src=\"images/ex3.1.png\" alt=\"ex3\" style=\"zoom: 40%;\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf24b52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('VeryBadLavaFloor-v0')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac451e5",
   "metadata": {},
   "source": [
    "### 3.1) Use one of the methods developed in the lab to compute the value function for the left diagram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490b94e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALUE ITERATION, il problema è un MDP: possiamo risolverlo optimalmente con VI \n",
    "\n",
    "def value_iteration(environment, maxiters=300, discount=0.9, max_error=1e-3):\n",
    "    \"\"\"\n",
    "    Performs the value iteration algorithm for a specific environment\n",
    "    \n",
    "    Args:\n",
    "        environment: OpenAI Gym environment\n",
    "        maxiters: timeout for the iterations\n",
    "        discount: gamma value, the discount factor for the Bellman equation\n",
    "        max_error: the maximum error allowd in the utility of any state\n",
    "        \n",
    "    Returns:\n",
    "        policy: 1-d dimensional array of action identifiers where index `i` corresponds to state id `i`\n",
    "    \"\"\"\n",
    "    \n",
    "    U_1 = [0 for _ in range(environment.observation_space.n)] # vector of utilities for states S\n",
    "    delta = 0 # maximum change in the utility o any state in an iteration\n",
    "    U = U_1.copy()\n",
    "    i = 0 \n",
    "    \n",
    "    while True: \n",
    "        i+=1\n",
    "        U = U_1.copy()\n",
    "        delta = 0 \n",
    "        \n",
    "        for state in range(environment.observation_space.n):\n",
    "            \n",
    "            val = [0 for _ in environment.actions]  \n",
    "            for action in environment.actions:\n",
    "                for next_state in range(environment.observation_space.n):\n",
    "                    val[action] += environment.T[state,action,next_state] * U[next_state]\n",
    "\n",
    "            if (env.grid[state] == \"G\")  or (env.grid[state] == \"P\"):\n",
    "                U_1[state] = environment.RS[state] \n",
    "            else : U_1[state] = environment.RS[state] + (discount * max(val))\n",
    "            \n",
    "            delta = max(abs(U_1[state] - U[state]),delta)\n",
    "            \n",
    "        if ( delta < ((max_error * (1 - discount))/discount) ) or (i > 300): break\n",
    "        \n",
    "    return values_to_policy(np.asarray(U), environment),U # automatically convert the value matrix U to a policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da933217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5093943765842497, 0.649585681261095, 0.7953620878466678, 1.0, 0.39844321783500447, 0.0, 0.48644001739269643, -1.0, 0.29628831545548107, 0.2538669984647952, 0.3447542300124158, 0.12987274656746348]\n",
      "[['R' 'R' 'R' 'L']\n",
      " ['U' 'L' 'U' 'L']\n",
      " ['U' 'R' 'U' 'L']]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('VeryBadLavaFloor-v0')\n",
    "policy, values = value_iteration(env)\n",
    "policy_render = np.vectorize(env.actions.get)(policy.reshape(env.rows, env.cols))\n",
    "print(values)\n",
    "print(policy_render)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8216357",
   "metadata": {},
   "source": [
    "### Results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66795ad",
   "metadata": {},
   "source": [
    "<img src=\"images/ex3.png\" alt=\"ex3\" style=\"zoom: 40%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b180254a",
   "metadata": {},
   "source": [
    "### 3.2) Consider the right diagram and focus on states $(2, 1)$. State whether the action reported in the right diagram (the blue one) represents the optimal action for that state. Motivate your answer with the code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebad340",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = {0: \"L\", 1: \"R\", 2: \"U\", 3: \"D\"}\n",
    "\n",
    "value_function = [0.50939438, 0.64958568, 0.79536209, 1, 0.39844322, 0, 0.48644002, -1, 0.29628832,  0.253867, 0.34475423, 0.12987275]\n",
    "id_start_state = 9\n",
    "gamma = 0.9\n",
    "\n",
    "values_ex = [0, 0, 0, 0]\n",
    "'''\n",
    "YOUR CODE HERE\n",
    "'''\n",
    "    \n",
    "print(np.asarray(values_ex))\n",
    "print(f'The correct action to perform should be: {actions[np.argmax(values_ex)]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611fa5fc",
   "metadata": {},
   "source": [
    "### 3.3) Compute the probability of ending in state (1, 3) if we execute the sequence of actons < Up, Up > from state (2, 2). Motivate your answer reporting the code and the solution printed. The following image shows a diagram to guide the solution process as a hint for you:\n",
    "\n",
    "\n",
    "<img src=\"images/example.png\" alt=\"ex3\" style=\"zoom: 30%;\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2369c26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_start_state = 10\n",
    "state = id_start_state\n",
    "actions = {0: \"L\", 1: \"R\", 2: \"U\", 3: \"D\"}\n",
    "\n",
    "prob = 0\n",
    "action = 2\n",
    "probs_fin = 0\n",
    "\n",
    "for next_state in range(env.observation_space.n):\n",
    "    \n",
    "    if env.T[state, action, next_state] == 0:\n",
    "        continue\n",
    "\n",
    "    # your code here\n",
    "    probs = env.T[state, action, next_state]\n",
    "\n",
    "    for second_next_state in range(env.observation_space.n):\n",
    "        \n",
    "        if env.T[next_state, action, second_next_state] == 0:\n",
    "            continue\n",
    "\n",
    "        if env.grid[second_next_state] == \"P\": #check that the state is (1,3) i.e., the black pit\n",
    "\n",
    "            # your code here\n",
    "            probs *= env.T[next_state, action, second_next_state]\n",
    "\n",
    "            print(f'{env.state_to_pos(state)} --> {env.state_to_pos(next_state)} --> {env.state_to_pos(second_next_state)}')\n",
    "            print(f'probs: {env.T[state, action, next_state]} --> {env.T[next_state, action, second_next_state]}')\n",
    "\n",
    "            # your code here\n",
    "            probs_fin += probs\n",
    "\n",
    "            print('================')\n",
    "            break\n",
    "    \n",
    "print()\n",
    "print('Probability: ', probs_fin)  \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455c0594",
   "metadata": {},
   "source": [
    "### 3.4) Consider the following environment where states $(0, 3)$ and $(1, 3)$ are terminal states with reward $+1$ and $−1$ respectively. Assume the transition model is the same one defined above and that the discounted factor is $0.9$ as above. However, now the agent pays $−0.4$ instead of $−0.04$ for every action, state and successor state.  Compute the new value function and the optimal policy for this new environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89237f75",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"images/env_3.4.png\" alt=\"ex3\" style=\"zoom: 40%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164ca1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('NiceLavaFloor-v0')\n",
    "policy, values = value_iteration(env,discount=0.9) \n",
    "policy_render = np.vectorize(env.actions.get)(policy.reshape(env.rows, env.cols))\n",
    "print(values)\n",
    "print(policy_render)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e4b4e0",
   "metadata": {},
   "source": [
    "### 3.5) Modify the value iteration parameters so that the policy allows the agent to reach the goal only starting from states $(0,1)$, $(0,2)$ and $(1,2)$, as reported in the image below . Motivate your answer.\n",
    "\n",
    "\n",
    "#### hint: given the negative reward the agent should care about the immediate reward and not the long-term reward.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfd75f5",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"images/result_ex3.5.png\" alt=\"ex3\" style=\"zoom: 40%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d7eed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['L' 'R' 'R' 'L']\n",
      " ['L' 'L' 'U' 'L']\n",
      " ['L' 'L' 'L' 'D']]\n"
     ]
    }
   ],
   "source": [
    "def value_iteration(environment, maxiters, discount, max_error):\n",
    "    \n",
    "    U_1 = [0 for _ in range(environment.observation_space.n)] # vector of utilities for states S\n",
    "    delta = 0 # maximum change in the utility o any state in an iteration\n",
    "    U = U_1.copy()\n",
    "    i = 0 \n",
    "    \n",
    "    while True: \n",
    "        i+=1\n",
    "        U = U_1.copy()\n",
    "        delta = 0 \n",
    "        \n",
    "        for state in range(environment.observation_space.n):\n",
    "            \n",
    "            val = [0 for _ in environment.actions]  \n",
    "            for action in environment.actions:\n",
    "                for next_state in range(environment.observation_space.n):\n",
    "                    val[action] += environment.T[state,action,next_state] * U[next_state]\n",
    "\n",
    "            if (env.grid[state] == \"G\")  or (env.grid[state] == \"P\"):\n",
    "                U_1[state] = environment.RS[state] \n",
    "            else : U_1[state] = environment.RS[state] + (discount * max(val))\n",
    "            \n",
    "            delta = max(abs(U_1[state] - U[state]),delta)\n",
    "            \n",
    "        if ( delta < ((max_error * (1 - discount))/discount) ) or (i > 300): break\n",
    "        \n",
    "    \n",
    "    return values_to_policy(np.asarray(U), environment),U # automatically convert the value matrix U to a policy\n",
    "\n",
    "env = gym.make('NiceLavaFloor-v0')\n",
    "policy, values =  value_iteration(env,300,0.1,1e-3) # maxiters va bene, diminuire il discount rende le REWARD FUTURE meno importanti, max error da non modificare credo \n",
    "policy_render = np.vectorize(env.actions.get)(policy.reshape(env.rows, env.cols))\n",
    "print(values)\n",
    "print(policy_render)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "planning-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3 (default, Mar 27 2019, 22:11:17) \n[GCC 7.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "33b088613692d9807fdfcb740d01861ee24b0449bcece85c75fff1cc70ad068a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
