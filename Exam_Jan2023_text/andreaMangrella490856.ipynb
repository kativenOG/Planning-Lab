{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6c87e41",
   "metadata": {},
   "source": [
    "# Exam Planning-Lab January 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0374992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "module_path = os.path.abspath(os.path.join('tools'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import gym, envs\n",
    "from utils.ai_lab_functions import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa5f24d",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65849ec8",
   "metadata": {},
   "source": [
    "<img src=\"images/ex1_.png\" alt=\"ex1\" style=\"zoom: 30%;\" />\n",
    "\n",
    "Consider the mobility graph in the figure above, where $S$ and $G$ are the start and goal positions, respectively, and $W_{i,j}$ represents the moving cost between the node $i$ and $j$. Assume all the $W_{i,j}$ are all integers and strictly positive. Assume the taxi can move in the following directions $<\"L\", \"R\", \"U\", \"D\">$. Note that there is a cost even in the case of an action that causes the agent to remain on the same starting state (e.g., if from state $0$ the agent performs action $L$ remains in $0$ with cost $W_{0,0}$).\n",
    "\n",
    "Answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdbdc94",
   "metadata": {},
   "source": [
    "### 1.1) Suppose there are no people in the grid world and a unitary cost for every $W_{i,j}$. Implement the A* search strategy using an admissible and consistent heuristic for the problem of finding a minimum-cost path (ignoring people requests). Show the result of the execution of your code and the total path cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b10a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please note that you need to modify the code developed in the lab to consider the cost of each displacement \n",
    "# (hint: when you create the child node) defined in the next cell and passed as a parameter to your method. \n",
    "\n",
    "def present_with_higher_cost(queue, node):\n",
    "    if node.state in queue:\n",
    "        if queue[node.state].value > node.value: \n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def astar_graph(environment, weights):\n",
    "    \"\"\"\n",
    "    A* Graph Search\n",
    "    \n",
    "    Args:\n",
    "        problem: OpenAI Gym environment\n",
    "        \n",
    "    Returns:\n",
    "        (path, time_cost, space_cost): solution as a path and stats.\n",
    "    \"\"\"\n",
    "\n",
    "    goalpos = environment.goalstate\n",
    "    print(goalpos)\n",
    "    explored = set()\n",
    "    frontier = PriorityQueue()\n",
    "    node = Node(environment.startstate,None)\n",
    "    print(node.state)\n",
    "    frontier.add(node)\n",
    "    \n",
    "    time_cost = 1\n",
    "    space_cost = 1\n",
    "    \n",
    "    while True:\n",
    "\n",
    "        if len(frontier) == 0:\n",
    "            return None, time_cost, space_cost\n",
    "            \n",
    "        space_cost = max(space_cost,len(frontier)+ len(explored))\n",
    "        parent = frontier.remove()\n",
    "        explored.add(parent.state) # addo a explored \n",
    "        if parent.state == goalpos: # END \n",
    "            return parent.pathcost, build_path(parent), time_cost, space_cost\n",
    "        \n",
    "        for action in environment.actions:\n",
    "            time_cost+=1\n",
    "            child_state= environment.sample(parent.state, action)\n",
    "            heuristic_value= Heu.l1_norm(environment.state_to_pos(child_state),environment.state_to_pos(goalpos))\n",
    "            child = Node(child_state,parent,parent.pathcost+ weights[parent.state,child_state],heuristic_value+ parent.pathcost + weights[parent.state,child_state])\n",
    "            if (child.state not in frontier) and (child.state not in explored): \n",
    "                frontier.add(child)\n",
    "            elif present_with_higher_cost(frontier, child):\n",
    "                frontier.replace(child) # PRIORITY QUEUE REPLACE FUNCTION, replaces the node with the same value, that functions as an ID   \n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60b6913",
   "metadata": {},
   "source": [
    "**Results of your solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10457a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This dictionary stores all the moving costs for the environment depicted above. In particular,\n",
    "each key (first element of the dictionary) is a tuple that consists of (start_state, goal_state). \n",
    "The value, i.e., the second element of the dictionary, is the moving cost.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "weights = {(0,1): 1, # start from state 0 and reach state 1, with cost 1\n",
    "           (1,0): 1, # start from state 1 and reach state 0, with cost 1\n",
    "           (1,2): 1,\n",
    "           (2,1): 1,\n",
    "           (2,5): 1,\n",
    "           (5,2): 1,\n",
    "           (5,4): 1,\n",
    "           (4,5): 1,\n",
    "           (4,1): 1,\n",
    "           (1,4): 1,\n",
    "           (4,3): 1,\n",
    "           (3,4): 1, \n",
    "           (3,0): 1,\n",
    "           (0,3): 1,\n",
    "           (0,0): 1, # this key-value pair encodes the situation in which the agent starts from state 0 \n",
    "                     # and performs an action that force it to remain in the same state, but accumulates \n",
    "                     # a unit cost.\n",
    "           (1,1): 1,\n",
    "           (2,2): 1,\n",
    "           (3,3): 1,\n",
    "           (4,4): 1,\n",
    "           (5,5): 1,\n",
    "          }\n",
    "\n",
    "\n",
    "env = gym.make('SmallMaze-v0')\n",
    "total_path_cost, solution, time, memory = astar(env,weights) \n",
    "print(\"Total path cost: \", total_path_cost)\n",
    "print(\"Solution A*: {}\".format(solution_2_string(solution, env)))\n",
    "print(\"NÂ° of nodes explored: {}\".format(time))\n",
    "print(\"Max nÂ° of nodes in memory: {}\\n\".format(memory))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043ec103",
   "metadata": {},
   "source": [
    "### 1.2) By implementing a function, verify if the heuristic chosen is admissible and consistent. Briefly motivate, with a comment at the beginning of the function, the process to check the consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcfc68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ANSWER:\n",
    "\n",
    "I chosed the l1 heuristic because during the lab session we discovered it is dominant\n",
    "with respect to l2 and chebyshev. \n",
    "An heuristic h1 is dominant with respect to an heuristic h2 if for every state h1(s) >= h2(s).\n",
    "If an heuristic is dominant it's better and resolves the problem in less time.\n",
    "I decided to give the proof only for consistency because if an heuristic is consistent it's also admissable.\n",
    "Consistency for an heuristic h1 can be defined as follows:\n",
    "    h1(s) <= c(s,a,s1) + h1(s1)\n",
    "\n",
    "\"\"\"\n",
    "def consistency_control(environment,weights):\n",
    "    goalpos = environment.state_to_pos(environment.goalstate)\n",
    "\n",
    "    for s in range(environment.observation_space.n):\n",
    "        if environment.grid[s] == \"W\": continue \n",
    "        for action in environment.actions:\n",
    "\n",
    "            s1 = environment.sample(s,action)\n",
    "            hn_s = Heu.l1_norm(environment.state_to_pos(s), goalpos)\n",
    "            hn_s1 = Heu.l1_norm(environment.state_to_pos(s1), goalpos)\n",
    "\n",
    "            if not (hn_s - hn_s1 <= weights[s,s1]): # action_cost = 1 \n",
    "                print(\"The L1 heuristic is not Consistent\")\n",
    "                return False \n",
    "\n",
    "    print(\"The L1 heuristic Manhattan distance is Consistent\")\n",
    "    return True \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "env = gym.make('SmallMaze-v0')\n",
    "consistency_control(env,weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80716464",
   "metadata": {},
   "source": [
    "### 1.3) Give a weight allocation such that the best solution achieved with A* (tree search version) strategy goes through all the nodes with passengers and reaches the node goal $ðº$ (i.e., that the agent is forced to go through node $id=3$ and node $id=2$, before reaching the goal node). Show with your code the results of the execution and the total path cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec090550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def astar_tree_search(environment, weights):\n",
    "    \"\"\"\n",
    "    A* Tree search\n",
    "    \n",
    "    Args:\n",
    "        problem: OpenAI Gym environment\n",
    "        \n",
    "    Returns:\n",
    "        (path, time_cost, space_cost): solution as a path and stats.\n",
    "    \"\"\"\n",
    "    goalpos = environment.goalstate\n",
    "    frontier = PriorityQueue()\n",
    "    node = Node(environment.startstate,None)\n",
    "    frontier.add(node)\n",
    "    \n",
    "    time_cost = 1\n",
    "    space_cost = 1\n",
    "    \n",
    "    while True:\n",
    "        if len(frontier) == 0:\n",
    "            return None, time_cost, space_cost\n",
    "        space_cost = max(space_cost,len(frontier))\n",
    "        \n",
    "        parent = frontier.remove()\n",
    "        if parent.state == goalpos:\n",
    "            return parent.pathcost, build_path(parent), time_cost, space_cost\n",
    "        \n",
    "        for action in environment.actions:\n",
    "            time_cost+=1\n",
    "            child_state= environment.sample(parent.state, action)\n",
    "            heuristic_value= Heu.l1_norm(environment.state_to_pos(child_state),environment.state_to_pos(goalpos))\n",
    "            child = Node(child_state,parent,parent.pathcost +weights[parent.state,child_state],heuristic_value+ parent.pathcost + weights[parent.state,child_state])\n",
    "            frontier.add(child)\n",
    "            if present_with_higher_cost(frontier, child):\n",
    "                frontier.replace(child) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3f525a",
   "metadata": {},
   "source": [
    "**Results of your solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9015ab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SmallMaze-v0')\n",
    "\n",
    "# your code for the weight allocation here\n",
    "weights[0,1] = 6 \n",
    "weights[4,1] = 6 \n",
    "\n",
    "total_path_cost, solution, time, memory = astar_tree_search(env,weights) # your code here\n",
    "print(\"Total path cost: {}\".format(total_path_cost))\n",
    "print(\"Solution A*(tree-search): {}\".format(solution_2_string(solution, env)))\n",
    "print(\"NÂ° of nodes explored: {}\".format(time))\n",
    "print(\"Max nÂ° of nodes in memory: {}\\n\".format(memory))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20a4d8c",
   "metadata": {},
   "source": [
    "### 1.4) Is the heuristic originally chosen still consistent in this case? Motivate your answer with the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59a6083",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The heuristic is still consistent because it does not overextimate the move_cost \n",
    "value defined by the weights.\n",
    "We can check it easily by doing the operation on the only two action changed:\n",
    "    \n",
    "    h(s)<= c(s,a,s1) + h(s1) \n",
    "    \n",
    "    1 <= 6+0  (for state 0 moving into 1 )\n",
    "    1 <= 6+0  (for state 4 moving into 1 )\n",
    "\n",
    "\"\"\"\n",
    "consistency_control(env,weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbec072d",
   "metadata": {},
   "source": [
    "### 1.5) Considering the task of transporting all passengers to the final location and the results obtained in the previous point, can you provide a different approach that finds a solution which guarantees least cost exploring less nodes? Motivate your answer by showing possible improvements with different strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c0c1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "ANSWER:\n",
    "\n",
    "By using A* with graph search it makes drastics improvements to the costs by adding\n",
    "an explored set for state values.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991722c6",
   "metadata": {},
   "source": [
    "**Results of your solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9078e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SmallMaze-v0')\n",
    "\n",
    "# your code for the weight allocation here (should be the same as the previous point)\n",
    "# So there is no need to reallocate them right ??? \n",
    "\n",
    "# WE ARE USING A* GRAPH SEARCH \n",
    "total_path_cost, solution, time, memory = astar_graph(env,weights) \n",
    "print(\"Total path cost: {}\".format(total_path_cost))\n",
    "print(\"Solution: {}\".format(solution_2_string(solution, env)))\n",
    "print(\"NÂ° of nodes explored: {}\".format(time))\n",
    "print(\"Max nÂ° of nodes in memory: {}\\n\".format(memory))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c885cbd7",
   "metadata": {},
   "source": [
    "### 1.6) Implement a Greedy Best First approach for this particular environment. With this strategy, can we obtain the desired solution (i.e., all the individuals transferred to the goal node $G$)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476b0222",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "There is no point in using greedy bfs because it doesnt care about the weights when computing \n",
    "the value of the node, in other words it cant reach the goal after taking all the passengers. \n",
    "\"\"\"\n",
    "\n",
    "def greedy_graph_search(environment,weights): \n",
    "\n",
    "    goalpos = environment.goalstate\n",
    "    print(goalpos)\n",
    "    explored = set()\n",
    "    frontier = PriorityQueue()\n",
    "    node = Node(environment.startstate,None)\n",
    "    print(node.state)\n",
    "    frontier.add(node)\n",
    "    \n",
    "    time_cost = 1\n",
    "    space_cost = 1\n",
    "    \n",
    "    while True:\n",
    "        if len(frontier) == 0:\n",
    "            return None, time_cost, space_cost\n",
    "            \n",
    "        space_cost = max(space_cost,len(frontier)+ len(explored))\n",
    "        parent = frontier.remove()\n",
    "        if parent.state == goalpos: \n",
    "            return parent.pathcost, build_path(parent), time_cost, space_cost\n",
    "        explored.add(parent.state) \n",
    "        \n",
    "        for action in environment.actions:\n",
    "            time_cost+=1\n",
    "            child_state= environment.sample(parent.state, action)\n",
    "            heuristic_value= Heu.chebyshev(environment.state_to_pos(child_state),environment.state_to_pos(goalpos))\n",
    "            child = Node(child_state,parent,parent.pathcost+ weights[parent.state,child_state],heuristic_value)\n",
    "            if (child.state not in frontier) and (child.state not in explored): \n",
    "                frontier.add(child)\n",
    "            elif present_with_higher_cost(frontier, child):\n",
    "                frontier.replace(child) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37871abe",
   "metadata": {},
   "source": [
    "**Results of your solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7de3a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SmallMaze-v0')\n",
    "\n",
    "\n",
    "# your code for the weight allocation here\n",
    "\n",
    "total_path_cost, solution, time, memory = greedy_graph_search(env,weights) \n",
    "print(\"Total path cost: {}\".format(total_path_cost))\n",
    "print(\"Solution Greedy Best First: {}\".format(solution_2_string(solution, env)))\n",
    "print(\"NÂ° of nodes explored: {}\".format(time))\n",
    "print(\"Max nÂ° of nodes in memory: {}\\n\".format(memory))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19347586",
   "metadata": {},
   "source": [
    "**Motivate your results:**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c85c89f",
   "metadata": {},
   "source": [
    "As explained above the code the Greedy bfs only cares about the value of the heuristic when calculating the value of the node. This makes it impossible for the algorithm to pick up all passengers before heading to the goal because it ***doesnt take in consideration the changes we applied to the weights***."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c789e14",
   "metadata": {},
   "source": [
    "### 1.7) Could you modify the environment maintaining the same weight allocation so that the manhattan distance is no loger consistent? Note that the problem should remain a single state problem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "507edd4e",
   "metadata": {},
   "source": [
    "if we add walls maybe ? \n",
    "COME BACK LATER TO THIS QUESTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0dce29",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5598937e",
   "metadata": {},
   "source": [
    "<img src=\"images/frozen.png\" alt=\"ex1\" style=\"zoom: 30%;\" />\n",
    "\n",
    "Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mainly frozen, but there are a few holes (highlighted with the letter ð» in the picture) where the ice has melted. Hence, you decide to use your robotic agent to retrieve the frisbee. However, if it steps into one of those holes, it will fall into the freezing water, and you lose both the robotic agent and the frisbee forever.\n",
    "\n",
    "Suppose the transition model for the environment is unknown. The return is discounted $\\gamma = 0.9$; the episode ends when the agent reaches the goal or falls into a hole. Moreover, the agent receives a reward of $+1$ if it retrieves the frisbee, i.e., it reaches the cell $(3,3)$, $0$ otherwise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9730b1",
   "metadata": {},
   "source": [
    "### 2.1) Given the environment reported above, implement an algorithm that can find the optimal policy, i.e., that achieves a 100% of success rate. Test the resulting policy using the provided code. ( For the action choice, you can use the explor_fun provided here below)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2d816a",
   "metadata": {},
   "source": [
    "**Utility functions provided for you:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f47b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explor_fun(env, q, state):\n",
    "    \n",
    "    # Choose the action with the highest value in the current state\n",
    "    if np.max(q[state]) > 0:\n",
    "        action = np.argmax(q[state])\n",
    "\n",
    "    # If there's no best action (only zeros), take a random one\n",
    "    else:\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "    return action\n",
    "\n",
    "\n",
    "def test_success(environment, qtable):\n",
    "    episodes = 100\n",
    "    nb_success = 0\n",
    "\n",
    "    # Evaluation\n",
    "    for _ in range(100):\n",
    "        state = environment.reset()\n",
    "        done = False\n",
    "\n",
    "        # Until the agent gets stuck or reaches the goal, keep training it\n",
    "        while not done:\n",
    "\n",
    "            #select the action using the explore function provided above\n",
    "            action = explor_fun(environment, qtable, state)\n",
    "\n",
    "            # perform the action in the environment\n",
    "            new_state, reward, done, info = environment.step(action)\n",
    "\n",
    "            # Update our current state\n",
    "            state = new_state\n",
    "\n",
    "            if done and reward > 0:\n",
    "                nb_success += reward\n",
    "\n",
    "    # Let's check our success rate!\n",
    "    print (f\"Success rate = {nb_success/episodes*100}%\")\n",
    "    \n",
    "    \n",
    "def execute_policy(environment, qtable):\n",
    "    import time\n",
    "    \n",
    "    state = environment.reset()\n",
    "    done = False\n",
    "    sequence = []\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        #select the action using the explore function provided above\n",
    "        action = explor_fun(environment, qtable, state)\n",
    "\n",
    "        # Add the action to the sequence\n",
    "        sequence.append(action)\n",
    "\n",
    "        # perform the action in the environment\n",
    "        new_state, reward, done, info = environment.step(action)\n",
    "\n",
    "        # Update our current state\n",
    "        state = new_state\n",
    "\n",
    "        # Update the render\n",
    "        environment.render()\n",
    "        time.sleep(1)\n",
    "\n",
    "    actions = [\"L\", \"D\", \"R\", \"U\"]\n",
    "    print(f\"Sequence = {[actions[a] for a in sequence]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d61d174",
   "metadata": {},
   "source": [
    "**Your solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "36054854",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We use sarsa because, by being on policy, it wont make the greediest choice like q-learning\n",
    "and will calculate the new best action based on epsilon greedy.\n",
    "I choosed SARSA because it's safer for the robot during the training stage and it'll be \n",
    "less likely that it will fall in the holes on the ice surface.\n",
    "\"\"\"\n",
    "\n",
    "def sarsa(environment, episodes, alpha, gamma, expl_func):\n",
    "\n",
    "    q = np.zeros((environment.observation_space.n, environment.action_space.n))  # Q(s, a)\n",
    "    rews = np.zeros(episodes)\n",
    "    lengths = np.zeros(episodes)\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        a = expl_func(environment,q,state)\n",
    "        while True:\n",
    "            next_state, reward, done, _ = env.step(a)  # Execute a step\n",
    "            a_1 = expl_func(environment,q,next_state)\n",
    "            rews[episode] += reward\n",
    "            lengths[episode]+= 1\n",
    "            q[state,a] = q[state,a] + alpha*(reward + gamma*q[next_state,a_1]  - q[state,a])\n",
    "            if done: \n",
    "                break\n",
    "            state = next_state\n",
    "            a = a_1\n",
    "\n",
    "    policy = q.argmax(axis=1) # q.argmax(axis=1) automatically extract the policy from the q table\n",
    "    return q,policy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a4a01a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "Sequence = ['D', 'R']\n",
      "Success rate = 1.0%\n"
     ]
    }
   ],
   "source": [
    "environment = gym.make(\"FrozenLake-v0\", is_slippery=False)\n",
    "\n",
    "# Hyperparameters\n",
    "episodes = 100000        #bTotal number of episodes\n",
    "alpha = 0.5            # Learning rate\n",
    "gamma = 0.9            # Discount factor\n",
    "\n",
    "qtable, policy = sarsa(environment,episodes,alpha,gamma,explor_fun)\n",
    "test_success(environment, qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b1bda7",
   "metadata": {},
   "source": [
    "**See the execution of your trained agent in the environment:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "06059ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "Sequence = ['D', 'L', 'L', 'D', 'R', 'R', 'U', 'R']\n"
     ]
    }
   ],
   "source": [
    "execute_policy(environment, qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359b9524",
   "metadata": {},
   "source": [
    "### 2.2) Consider and environment with states {A, B, C, D}, actions {r, l} where states {A, D} are terminal. Consider the following sequence of learning episodes:\n",
    "* E1: (B, r, C, âˆ’1),(C, r, C, âˆ’1),(C, r, D, +1)\n",
    "* E2: (B, r, C, âˆ’1),(C, r, D, +1)\n",
    "* E3: (B, l, A, +5)\n",
    "* E4: (B, l, B, âˆ’1),(B, l, B, âˆ’1),(B, l, A, +5)\n",
    "\n",
    "### Build an estimation of the transition model T using the code provided below.\n",
    "\n",
    "#### hint: The transition model T is a probability distribution that describes the likelihood of transitioning from one state to another given an action. We can estimate T from a sequence of learning episodes by counting the number of times each transition occurs and dividing by the total number of transitions for a given state-action pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "148f6626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prob of starting in B compute action r and finish in C is:  1.0\n",
      "Prob of starting in B compute action l and finish in A is:  0.5\n",
      "Prob of starting in B compute action l and finish in B is:  0.5\n",
      "Prob of starting in C compute action r and finish in C is:  0.3333333333333333\n",
      "Prob of starting in C compute action r and finish in D is:  0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "episodes = {1: [('B', 'r', 'C', -1), ('C', 'r', 'C', -1),('C', 'r', 'D', 1)], \n",
    "            2: [('B', 'r', 'C', -1), ('C', 'r', 'D', 1)], \n",
    "            3: [('B', 'l', 'A', 5)], \n",
    "            4: [('B', 'l', 'B', -1),('B', 'l', 'B', -1),('B', 'l', 'A', 5)]\n",
    "           }\n",
    "\n",
    "\n",
    "#T:\n",
    "\n",
    "count_brc = 0\n",
    "tot_count_br = 0\n",
    "\n",
    "count_bla = 0\n",
    "count_blb = 0\n",
    "tot_count_bl = 0\n",
    "\n",
    "count_crc = 0\n",
    "tot_count_cr = 0\n",
    "\n",
    "count_crd = 0\n",
    "\n",
    "\n",
    "for episode, values in episodes.items():\n",
    "    for tuple_t in values:\n",
    "\n",
    "        # BRC \n",
    "        if tuple_t[:3] == ('B', 'r', 'C'): count_brc += 1\n",
    "        # BLA \n",
    "        if tuple_t[:3] == ('B', 'l', 'A'): count_bla += 1\n",
    "        # BlB \n",
    "        if tuple_t[:3] == ('B', 'l', 'B'): count_blb += 1\n",
    "        # CRC \n",
    "        if tuple_t[:3] == ('C', 'r', 'C'): count_crc += 1\n",
    "        # CRD \n",
    "        if tuple_t[:3] == ('C', 'r', 'D'): count_crd += 1\n",
    "\n",
    "        # BR \n",
    "        if tuple_t[:2] == ('B', 'r'): tot_count_br += 1\n",
    "        # BL \n",
    "        if tuple_t[:2] == ('B', 'l'): tot_count_bl += 1\n",
    "        # CR \n",
    "        if tuple_t[:2] == ('C', 'r'): tot_count_cr += 1\n",
    "\n",
    "\n",
    "t_br,t_bl,t_cr= [0 for _ in range(4)],[0 for _ in range(4)],[0 for _ in range(4)]\n",
    "t_br[2] = (count_brc/tot_count_br)\n",
    "t_bl[0] = (count_bla/tot_count_bl)\n",
    "t_bl[1] = (count_blb/tot_count_bl)\n",
    "t_cr[2] = (count_crc/tot_count_cr)\n",
    "t_cr[3] = (count_crd/tot_count_cr)\n",
    "\n",
    "print('Prob of starting in B compute action r and finish in C is: ', count_brc/tot_count_br)\n",
    "print('Prob of starting in B compute action l and finish in A is: ', count_bla/tot_count_bl)\n",
    "print('Prob of starting in B compute action l and finish in B is: ', count_blb/tot_count_bl)\n",
    "print('Prob of starting in C compute action r and finish in C is: ', count_crc/tot_count_cr)\n",
    "print('Prob of starting in C compute action r and finish in D is: ', count_crd/tot_count_cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd800bed",
   "metadata": {},
   "source": [
    "### 2.3) Compute V(s) for all non-terminal states by using a direct evaluation approach (i.e., without considering the structure of the bellman equation).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f89c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = {1: [('B', 'r', 'C', -1), ('C', 'r', 'C', -1),('C', 'r', 'D', 1)], \n",
    "            2: [('B', 'r', 'C', -1), ('C', 'r', 'D', 1)], \n",
    "            3: [('B', 'l', 'A', 5)], \n",
    "            4: [('B', 'l', 'B', -1),('B', 'l', 'B', -1),('B', 'l', 'A', 5)]\n",
    "           }\n",
    "\n",
    "v = {'A': 5, 'B': 0, 'C': 0, 'D': 1}\n",
    "\n",
    "count_b = 0\n",
    "reward_b = 0\n",
    "count_c = 0\n",
    "reward_c = 0\n",
    "\n",
    "for episode, values in episodes.items():\n",
    "    for tuple_t in values:\n",
    "            \n",
    "        if tuple_t[0] == ('C'):\n",
    "            #c= True  NO NEED \n",
    "            reward_c += tuple_t[-1]\n",
    "            count_c += 1\n",
    "        else:\n",
    "            #if tuple_t[0] == ('B'):  A and B are the only non-terminal states so they are alwaays one of the two starting states\n",
    "            count_b += 1\n",
    "            reward_b += tuple_t[-1]\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "v['B'] = None \n",
    "v['C'] = None  \n",
    "\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3dc884",
   "metadata": {},
   "source": [
    "### 2.4) Consider and environment with states {A, B, C, D}, actions {r, l} where states {A, D} are terminal. Consider the following different sequence of learning episodes:\n",
    "* E1: (B, r, C, âˆ’1),(C, r, C, âˆ’1),(C, r, D, +1)\n",
    "* E2: (B, r, C, âˆ’1),(C, r, D, +1)\n",
    "* E3: (B, l, A, +5)\n",
    "* E4: (B, l, B, âˆ’1),(B, l, B, âˆ’1),(B, l, A, +5)\n",
    "### Compute v(s) for all non-terminal states by using a sample-based evaluation approach assuming $\\alpha$ = .1 and $\\gamma$ = 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e876f091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 5, 'B': 1.4651930390000003, 'C': 0.28, 'D': 1}\n"
     ]
    }
   ],
   "source": [
    "# I just added the formula for the sample based approach in the for for each tuple \n",
    "\n",
    "episodes = {1: [('B', 'r', 'C', -1), ('C', 'r', 'C', -1),('C', 'r', 'D', 1)], \n",
    "            2: [('B', 'r', 'C', -1), ('C', 'r', 'D', 1)], \n",
    "            3: [('B', 'l', 'A', 5)], \n",
    "            4: [('B', 'l', 'B', -1),('B', 'l', 'B', -1),('B', 'l', 'A', 5)]}\n",
    "v = {'A': 5, 'B': 0, 'C': 0, 'D': 1}\n",
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "\n",
    "for episode, values in episodes.items():\n",
    "    for tuple_t in values:\n",
    "       v[tuple_t[0]] = (1-alpha) * v[tuple_t[0]] + alpha*(tuple_t[-1] + gamma*v[tuple_t[2]])\n",
    "       \n",
    "        \n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7429362",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad5e7a1",
   "metadata": {},
   "source": [
    "<img src=\"images/env3.png\" alt=\"ex1\" style=\"zoom: 30%;\" />\n",
    "\n",
    "Consider the environment displayed in Figure above, where states $(0, 3)$ and $(1, 3)$ are terminal states with reward $+1$ and $âˆ’1$ respectively. The agent can move in the four directions. The transition model states that for every state and action the agent has $0.8$ chances of moving in the chosen direction and $0.1$ chances to move in the othogonal directions.The reward model states that for every state, action and successor state the agent pays $âˆ’0.01$. Assume that the discount factor is set to $\\gamma = 0.9$. Answer the following questions: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd4c757",
   "metadata": {},
   "source": [
    "### 3.1) Use one of the methods developed in the lab to compute the optimal policy and the value function. Print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79df980",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "your code here\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498f4fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('VeryBadLavaFloor-v0')\n",
    "env.render()\n",
    "print()\n",
    "\n",
    "policy, values = None, None# your code here\n",
    "policy_render = np.vectorize(env.actions.get)(policy.reshape(env.rows, env.cols))\n",
    "print(values)\n",
    "print(policy_render)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8658e84",
   "metadata": {},
   "source": [
    "### 3.2) Consider the optimal policy obtained in the previous exercise and focus on states $(2, 1)$. State why in that state the policy selects the \"U\" action, and what is the $\\Delta$  to instead choosing an \"L\" action  (i.e., $Q((2,1),RIGHT)$ - $Q((2,1),LEFT)$). Motivate your answer with the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718f935d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "your code here\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fbaa9c",
   "metadata": {},
   "source": [
    "### 3.3) Consider now the starting state $S$,  cell $(0,0)$. What is the $\\Delta$ between the optimal action and the second-best one? Print both the action and the $\\Delta$ value. The code should be parametric, i.e., if we change the initial state, it must return the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf725ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "your code here\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5964e9c",
   "metadata": {},
   "source": [
    "### 3.4) Consider the following environment where states $(0, 3)$ and $(1, 3)$ are terminal states with reward $+1$ and $âˆ’1$ respectively. The transition model is the same one defined above, however the agent now gets a $+0.5$ for every action perfomed in the environment. Is it possible to get an optimal policy that allows an agent to start from state $(0,0)$ and reach state $(0,3)$? Motivate your answer with the code.\n",
    "\n",
    "<img src=\"images/env3.png\" alt=\"ex1\" style=\"zoom: 30%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f2533a07",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1417855668.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_14037/1417855668.py\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    policy, values = #your code here\u001b[0m\n\u001b[0m                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('NiceLavaFloor-v0')\n",
    "env.render()\n",
    "print()\n",
    "\n",
    "policy, values = #your code here\n",
    "policy_render = np.vectorize(env.actions.get)(policy.reshape(env.rows, env.cols))\n",
    "print(values)\n",
    "print(policy_render)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62760fed",
   "metadata": {},
   "source": [
    "### 3.5) Compute the probability of ending in state (1, 3) if we execute the sequence of actons < Right, Up > from state (0, 2). Motivate your answer reporting the code and the solution printed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "752cec6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 2) --> (1, 2) --> (1, 3)\n",
      "probs: 0.1 --> 0.1\n",
      "================\n",
      "\n",
      "Probability:  0.01\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The probability result is 0.01, I computed it by just cycling \n",
    "in all states, from the starting position and calculating \n",
    "the probability to go in the right direction.\n",
    "We cycle 2 times trough states, once for the first action and another time for the second.\n",
    "We exit from the loop once we reach the desired state with our two actions.\n",
    "\n",
    "\"\"\"\n",
    "env = gym.make('NiceLavaFloor-v0')\n",
    "\n",
    "id_start_state = env.pos_to_state(0,2) # start state \n",
    "id_end_state = env.pos_to_state(1,3) # end state \n",
    "state = id_start_state\n",
    "actions = {0: \"L\", 1: \"R\", 2: \"U\", 3: \"D\"}\n",
    "\n",
    "prob = 0\n",
    "action_1 = 1\n",
    "action_2 = 2\n",
    "probs_fin = 0\n",
    "\n",
    "for next_state in range(env.observation_space.n):\n",
    "    if env.T[state, action_1, next_state] == 0:\n",
    "        continue\n",
    "\n",
    "    probs = env.T[state, action_1, next_state]\n",
    "    \n",
    " \n",
    "    for second_next_state in range(env.observation_space.n):\n",
    "        if env.T[next_state, action_2, second_next_state] == 0:\n",
    "            continue\n",
    "\n",
    "        if second_next_state == id_end_state:\n",
    "    \n",
    "            probs *= env.T[next_state, action_2, second_next_state]\n",
    "\n",
    "            print(f'{env.state_to_pos(state)} --> {env.state_to_pos(next_state)} --> {env.state_to_pos(second_next_state)}')\n",
    "            print(f'probs: {env.T[state, action_1, next_state]} --> {env.T[next_state, action_2, second_next_state]}')\n",
    "            probs_fin += probs\n",
    "\n",
    "            print('================')\n",
    "            break\n",
    "    \n",
    "print()\n",
    "print('Probability: ', round(probs_fin,2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "planning-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "33b088613692d9807fdfcb740d01861ee24b0449bcece85c75fff1cc70ad068a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
