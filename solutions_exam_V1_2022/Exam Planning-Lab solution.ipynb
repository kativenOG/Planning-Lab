{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c707f35e",
   "metadata": {},
   "source": [
    "# Exam Planning-Lab with solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81dac69",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233976dd",
   "metadata": {},
   "source": [
    "Consider the following environment:\n",
    "\n",
    "<img src=\"images/env1.png\" alt=\"ex1\" style=\"zoom: 40%;\" />\n",
    "\n",
    "The agent starts in cell $(0, 0)$ and has to reach the treasure in $(2, 3)$. In addition to the walls of the previous environments, the floor is covered with lava, however the agent can resist to high temperature and it can use the heat to recharge its batteries, hence it receives a positive reward for being on a lava cell. The environment is deterministic and the agent must avoid the black pits of death (cells $(0,3)$, $(1, 3)$, $(1,1)$). \n",
    "Assume that you do not have access to the motion model and to reward and that the problem is undiscounted (i.e., $\\gamma$ =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5be1e677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['S' 'L' 'L' 'P']\n",
      " ['L' 'P' 'L' 'P']\n",
      " ['L' 'L' 'L' 'G']]\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "module_path = os.path.abspath(os.path.join('tools'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import gym, envs\n",
    "from utils.ai_lab_functions import *\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm as tqdm\n",
    "import numpy as np\n",
    "\n",
    "#POSSIBLE SOLUTION Q-LEARNING OR SARSA  \n",
    "def epsilon_greedy(q, state, epsilon):\n",
    "    \"\"\"\n",
    "    Epsilon-greedy action selection function\n",
    "    \n",
    "    Args:\n",
    "        q: q table\n",
    "        state: agent's current state\n",
    "        epsilon: epsilon parameter\n",
    "    \n",
    "    Returns:\n",
    "        action id\n",
    "    \"\"\"\n",
    "    an = q.shape[1]\n",
    "    probs = np.full(an, epsilon / an)\n",
    "    probs[q[state].argmax()] += 1 - epsilon\n",
    "    return np.random.choice(an, p=probs)\n",
    "\n",
    "\n",
    "env = gym.make('LavaFloor-v0')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc195b2",
   "metadata": {},
   "source": [
    "### 1.1) Given the environment reported above, find a policy with a suitable algorithm of your choice. Print the resulting policy (you can use the provided code for this)Â¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dad1e21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def q_learning(environment, episodes, alpha, gamma, expl_func, expl_param):\n",
    "    \"\"\"\n",
    "    Performs the Q-Learning algorithm for a specific environment\n",
    "    \n",
    "    Args:\n",
    "        environment: OpenAI Gym environment\n",
    "        episodes: number of episodes for training\n",
    "        alpha: alpha parameter\n",
    "        gamma: gamma parameter\n",
    "        expl_func: exploration function (epsilon_greedy, softmax)\n",
    "        expl_param: exploration parameter (epsilon, T)\n",
    "    \n",
    "    Returns:\n",
    "        (policy, rewards, lengths): final policy, rewards for each episode [array], length of each episode [array]\n",
    "    \"\"\"\n",
    "    \n",
    "    q = np.zeros((environment.observation_space.n, environment.action_space.n))  # Q(s, a)\n",
    "    rews = np.zeros(episodes)\n",
    "    lengths = np.zeros(episodes)\n",
    "    env = environment\n",
    "    for e in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        leng = 0\n",
    "        while not done:\n",
    "           #choose  action\n",
    "            action = expl_func(q, state, expl_param)\n",
    "            \n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            rews[e] += reward\n",
    "            \n",
    "            max_ = q[new_state,action]\n",
    "            for a in range(environment.action_space.n):\n",
    "                if q[new_state,a] > max_:\n",
    "                    max_ = q[new_state,a]\n",
    "                \n",
    "            q[state,action] = q[state,action] + alpha * (reward + gamma *max_  - q[state,action])\n",
    "            state = new_state\n",
    "            leng+=1\n",
    "        \n",
    "        lengths[e] = leng\n",
    "            \n",
    "    policy = q.argmax(axis=1)\n",
    "    return policy, rews, lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ce36292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy:\n",
      " [['L' 'L' 'L' 'L']\n",
      " ['U' 'L' 'L' 'L']\n",
      " ['U' 'L' 'L' 'L']] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "alpha = .3\n",
    "gamma = 0.99\n",
    "\n",
    "epsilon = .1\n",
    "\n",
    "# Q-Learning epsilon greedy\n",
    "env = gym.make('LavaFloor-v0')\n",
    "policy, rewards, lengths = q_learning(env, episodes, alpha, gamma, epsilon_greedy, epsilon)\n",
    "print(\"Policy:\\n {} \\n\".format(np.vectorize(env.actions.get)(policy.reshape(env.shape))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e41eea25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(environment, episodes, alpha, gamma, expl_func, expl_param):\n",
    "    \"\"\"\n",
    "    Performs the SARSA algorithm for a specific environment\n",
    "    \n",
    "    Args:\n",
    "        environment: OpenAI gym environment\n",
    "        episodes: number of episodes for training\n",
    "        alpha: alpha parameter\n",
    "        gamma: gamma parameter\n",
    "        expl_func: exploration function (epsilon_greedy, softmax)\n",
    "        expl_param: exploration parameter (epsilon, T)\n",
    "    \n",
    "    Returns:\n",
    "        (policy, rewards, lengths): final policy, rewards for each episode [array], length of each episode [array]\n",
    "    \"\"\"\n",
    "\n",
    "    q = np.zeros((environment.observation_space.n, environment.action_space.n))  # Q(s, a)\n",
    "    rews = np.zeros(episodes)\n",
    "    lengths = np.zeros(episodes)\n",
    "    for i in range(episodes):\n",
    "        state = environment.reset()  # Reset the environment\n",
    "        action = expl_func(q, state, expl_param)  # Select first action\n",
    "        el = 0\n",
    "        \n",
    "        while True:\n",
    "            next_state, reward, done, _ = environment.step(action)  # Execute a step\n",
    "            action_next = expl_func(q, next_state, expl_param)  # Select next action\n",
    "            q[state, action] += alpha * (reward + gamma * q[next_state, action_next] - q[state, action])  # Temporal difference\n",
    "            rews[i] += reward\n",
    "            el += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state  # Update state\n",
    "            action = action_next  # Update action\n",
    "            \n",
    "        lengths[i] = el\n",
    "        \n",
    "    policy = q.argmax(axis=1) # q.argmax(axis=1) automatically extract the policy from the q table\n",
    "    return policy, rews, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee0dd305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy:\n",
      " [['U' 'L' 'L' 'L']\n",
      " ['U' 'L' 'U' 'L']\n",
      " ['L' 'L' 'L' 'L']] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "episodes = 500\n",
    "alpha = .3\n",
    "gamma = .99\n",
    "epsilon = .1\n",
    "\n",
    "# SARSA epsilon greedy\n",
    "env = gym.make('LavaFloor-v0')\n",
    "policy, rewards, lengths = sarsa(env, episodes, alpha, gamma, epsilon_greedy, epsilon)\n",
    "print(\"Policy:\\n {} \\n\".format(np.vectorize(env.actions.get)(policy.reshape(env.shape))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72ae25a",
   "metadata": {},
   "source": [
    "### 1.2) Find a way to force the agent to choose the shortest path towards the goal. You can change these parameters: episodes, alpha, gamma exploration function and exploration parameter. \n",
    "\n",
    "##### Hint: you should tune a parameter to consider the immediate reward rather than the long-term one... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86059fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy:\n",
      " [['D' 'L' 'L' 'L']\n",
      " ['D' 'L' 'D' 'L']\n",
      " ['R' 'R' 'R' 'L']] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "episodes = 300\n",
    "alpha = .3\n",
    "\n",
    "# TUNING DISCOUNT FACTOR\n",
    "gamma = .1\n",
    "epsilon = .3\n",
    "\n",
    "# Q-Learning epsilon greedy\n",
    "env = gym.make('LavaFloor-v0')\n",
    "p, r, l = q_learning(env, episodes, alpha, gamma, epsilon_greedy, epsilon)\n",
    "print(\"Policy:\\n {} \\n\".format(np.vectorize(env.actions.get)(p.reshape(env.shape))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dec17a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy:\n",
      " [['D' 'L' 'L' 'L']\n",
      " ['D' 'L' 'D' 'L']\n",
      " ['R' 'R' 'R' 'L']] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "episodes = 1000\n",
    "alpha = .4\n",
    "gamma = .1\n",
    "epsilon = .3\n",
    "\n",
    "# SARSA epsilon greedy\n",
    "env = gym.make('LavaFloor-v0')\n",
    "policy, rewards, lengths = sarsa(env, episodes, alpha, gamma, epsilon_greedy, epsilon)\n",
    "print(\"Policy:\\n {} \\n\".format(np.vectorize(env.actions.get)(policy.reshape(env.shape))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbff4e4",
   "metadata": {},
   "source": [
    "### 1.3) Consider and environment with states {A, B, C, D}, actions {r, l} where states {A, D} are terminal. Consider the following sequence of learning episodes:\n",
    "* E1: (B, r, C, â1),(C, r, C, â1),(C, r, D, +1)\n",
    "* E2: (B, r, C, â1),(C, r, D, +1)\n",
    "* E3: (B, l, A, +5)\n",
    "* E4: (B, l, B, â1),(B, l, B, â1),(B, l, A, +5)\n",
    "### Compute v(s) for all non-terminal states by using a sample-based evaluation approach (i.e., computing the values with the function reported below). Assume $\\alpha$ = .5 and $\\gamma$ = 1.\n",
    "### $V(s) = (1- \\alpha) \\cdot V(s) + \\alpha \\cdot (r + \\gamma \\cdot V'(s))$ \n",
    "### where $s$ is the state under consideration (first element of each tuple), $s'$  (third element of each tuple ) the state reached by starting from $s$ and performing the action $a$ (second element of each tuple). And finally, $r$ is the reward (last element of each tuple)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "017a0627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 5, 'B': 6.90625, 'C': 1.375, 'D': 1}\n"
     ]
    }
   ],
   "source": [
    "episodes = {1: [('B', 'r', 'C', -1), ('C', 'r', 'C', -1),('C', 'r', 'D', 1)], \n",
    "            2: [('B', 'r', 'C', -1), ('C', 'r', 'D', 1)], \n",
    "            3: [('B', 'l', 'A', 5)], \n",
    "            4: [('B', 'l', 'B', -1),('B', 'l', 'B', -1),('B', 'l', 'A', 5)]}\n",
    "v = {'A': 5, 'B': 0, 'C': 0, 'D': 1}\n",
    "alpha = 0.5\n",
    "gamma = 1\n",
    "\n",
    "for episode, values in episodes.items():\n",
    "    for tuple in values:\n",
    "        v[tuple[0]] = (1-alpha)*v[tuple[0]] + alpha*(tuple[-1]+ gamma*v[tuple[2]])\n",
    "      \n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfc8f7c",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc5f593",
   "metadata": {},
   "source": [
    "Consider the figure below where **S=$(1,2)$** and **G=$(3,1)$** are the starting and goal positions respectively. Consider the problem of finding a minimum cost path from S to G assuming the agent can move in the four directions (if there is no\n",
    "obstacle) and that each movement has a unitary cost. The environment is deterministic. Answer the following questions:\n",
    "\n",
    "<img src=\"images/ex2.png\" alt=\"ex2\" style=\"zoom: 40%;\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed0ba51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['C' 'C' 'C' 'C' 'C']\n",
      " ['C' 'C' 'S' 'W' 'C']\n",
      " ['C' 'W' 'W' 'W' 'C']\n",
      " ['C' 'G' 'C' 'C' 'C']]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('SmallMaze-v0')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7062b99",
   "metadata": {},
   "source": [
    "### 2.1) Verify by using the code developed in the lab that the Manhattan distance (l1_norm) is a  *consistent* heuristic in this environment. In particular, you should implement a function that checks whether for every couple of state $(s,s')$ (where $s'$ is a successor state of $s$), the consistency condition is verified. The function should return true if the heuristic is consistent and false otherwise. Recall that every action has a cost of 1...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bcb915",
   "metadata": {},
   "source": [
    "#### my solution:\n",
    "\n",
    "a heuristic is considered consistent if for all (n, n'), h(n) <= h(n') + c(n,a,n'). In this case, having unit cost per displacement, it is easy to see that the manhattan distance (l1_norm) heuristic is consistent since for each node and its successor we have h(n) - h(n') <= 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "937f429d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The heuristic is consistent!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_euristic(environment):\n",
    "    goalpos = environment.state_to_pos(environment.goalstate)\n",
    "    for s in range(environment.observation_space.n): #for every state\n",
    "        #print('s = {}'.format(environment.state_to_pos(s)))\n",
    "        \n",
    "        # skip the check if the state is a wall\n",
    "        if env.grid[s] == \"W\":\n",
    "            continue\n",
    "            \n",
    "        for action in range(environment.action_space.n): #Look around\n",
    "            #print('a = {}'.format(action))\n",
    "            \n",
    "            # get next state\n",
    "            s1 = environment.sample(s, action)\n",
    "            \n",
    "            #compute heuristic for s\n",
    "            Hn = Heu.l1_norm(environment.state_to_pos(s), goalpos) # node value (heuristic)\n",
    "            \n",
    "            #compute heuristic for s'\n",
    "            Hn1 = Heu.l1_norm(environment.state_to_pos(s1), goalpos) # node value (heuristic)\n",
    "            \n",
    "            # check consistency\n",
    "            test = ((Hn - Hn1) <= 1)\n",
    "            \n",
    "            if not test:\n",
    "                print(\"The heuristic is not consistent!\")\n",
    "                print('s = {} \\t s1 = {}'.format(environment.state_to_pos(s),environment.state_to_pos(s1)))\n",
    "                print('h(n)= {} \\t h(n1) = {} \\t res = {}'.format(Hn,Hn1,test))\n",
    "                return False\n",
    "            \n",
    "          \n",
    "        print(\"The heuristic is consistent!\")\n",
    "        return True\n",
    "\n",
    "\n",
    "env = gym.make('SmallMaze-v0')\n",
    "check_euristic(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97399378",
   "metadata": {},
   "source": [
    "### 2.2) Consider the A* algorithm and assume want to achieve optimality. Based on the consistent heuristics of Section 2.1, state whether it is best to use a graph search or tree search strategy. Motivate your answer and show the results of A* execution and the differences between the two versions in terms of the computed solution, number of nodes generated and maximum number of nodes in memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217679b5",
   "metadata": {},
   "source": [
    "Since we have a consistent heuristic we have to choose a graph search version to achieve optimality. We know only if tree search + admissible heuristic or graph search + consistent heuristic => optimality of A*. In all other situations, we cannot guarantee optimality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c74ec4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution A*(tree-search): [(1, 1), (1, 0), (2, 0), (3, 0), (3, 1)]\n",
      "NÂ° of nodes explored: 125\n",
      "Max nÂ° of nodes in memory: 94\n",
      "\n",
      "=================================================================\n",
      "\n",
      "Solution A*(graph-search): [(1, 1), (1, 0), (2, 0), (3, 0), (3, 1)]\n",
      "NÂ° of nodes explored: 29\n",
      "Max nÂ° of nodes in memory: 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def present_with_higher_cost(queue, node):\n",
    "    if node.state in queue:\n",
    "        if queue[node.state].value > node.value: \n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def astar_tree_search(environment):\n",
    "    \"\"\"\n",
    "    A* Tree search\n",
    "    \n",
    "    Args:\n",
    "        problem: OpenAI Gym environment\n",
    "        \n",
    "    Returns:\n",
    "        (path, time_cost, space_cost): solution as a path and stats.\n",
    "    \"\"\"\n",
    "\n",
    "    goalpos = environment.state_to_pos(environment.goalstate)\n",
    "    queue = PriorityQueue()\n",
    "    queue.add(Node(environment.startstate, value=Heu.l1_norm(environment.state_to_pos(environment.startstate), goalpos)))\n",
    "    time_cost = 1\n",
    "    space_cost = 1\n",
    "    \n",
    "    while True:        \n",
    "        if queue.is_empty(): \n",
    "            return None, time_cost, space_cost\n",
    "    \n",
    "        node = queue.remove()\n",
    "        if node.state == environment.goalstate: \n",
    "            return build_path(node), time_cost, space_cost\n",
    "        \n",
    "        for action in range(environment.action_space.n): #Look around\n",
    "            child_state = environment.sample(node.state, action)\n",
    "            child = Node( \n",
    "                          child_state, # node state\n",
    "                          node, # node parent\n",
    "                          node.pathcost + 1, # incremental path cost\n",
    "                          node.pathcost + 1 + Heu.l1_norm(environment.state_to_pos(child_state), goalpos) # node value (heuristic)\n",
    "                         ) \n",
    "            \n",
    "            time_cost += 1\n",
    "            queue.add(child)\n",
    "            \n",
    "        space_cost = max(space_cost, len(queue))\n",
    "\n",
    "def astar_graph_search(environment):\n",
    "    \"\"\"\n",
    "    A* Graph Search\n",
    "    \n",
    "    Args:\n",
    "        problem: OpenAI Gym environment\n",
    "        \n",
    "    Returns:\n",
    "        (path, time_cost, space_cost): solution as a path and stats.\n",
    "    \"\"\"\n",
    "    \n",
    "    goalpos = environment.state_to_pos(environment.goalstate)\n",
    "    queue = PriorityQueue()\n",
    "    queue.add(Node(environment.startstate, value=Heu.l1_norm(environment.state_to_pos(environment.startstate), goalpos)))\n",
    "    time_cost = 1\n",
    "    space_cost = 1\n",
    "    explored = set()\n",
    "    \n",
    "    while True:\n",
    "        if queue.is_empty(): return None, time_cost, space_cost\n",
    "        \n",
    "        node = queue.remove()\n",
    "        if node.state == environment.goalstate: \n",
    "            return build_path(node), time_cost, space_cost\n",
    "        explored.add(node.state)\n",
    "            \n",
    "        for action in range(environment.action_space.n):  #Look around\n",
    "            child_state = environment.sample(node.state, action)\n",
    "            child = Node( \n",
    "                      child_state, # node state\n",
    "                      node, # node parent\n",
    "                      node.pathcost + 1, # incremental path cost\n",
    "                      node.pathcost + 1 + Heu.l1_norm(environment.state_to_pos(child_state), goalpos) # node value (heuristic)\n",
    "                     ) \n",
    "            time_cost += 1\n",
    "            \n",
    "            if child.state not in queue and child.state not in explored:\n",
    "                #if child.state == environment.goalstate: return build_path(node), time_cost, space_cost\n",
    "                queue.add(child)\n",
    "            elif present_with_higher_cost(queue, child):\n",
    "                queue.replace(child)\n",
    "                \n",
    "        space_cost = max(space_cost, len(queue) + len(explored))\n",
    "\n",
    "\n",
    "env = gym.make('SmallMaze-v0')\n",
    "solution_ts, time_ts, memory_ts = astar_tree_search(env)\n",
    "solution_gs, time_gs, memory_gs = astar_graph_search(env)\n",
    "print(\"Solution A*(tree-search): {}\".format(solution_2_string(solution_ts, env)))\n",
    "print(\"NÂ° of nodes explored: {}\".format(time_ts))\n",
    "print(\"Max nÂ° of nodes in memory: {}\\n\".format(memory_ts))\n",
    "print('='*65)\n",
    "print(\"\\nSolution A*(graph-search): {}\".format(solution_2_string(solution_gs, env)))\n",
    "print(\"NÂ° of nodes explored: {}\".format(time_gs))\n",
    "print(\"Max nÂ° of nodes in memory: {}\\n\".format(memory_gs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f177b11c",
   "metadata": {},
   "source": [
    "### 2.3) Let us consider BFS, show the path of the optimal solution (avoiding the repetition of states) to achieve the goal. In this scenario can we guarantee that the returned solution is the least cost one? If we used Iterative Deepening Search(IDS) with the same methodology used for BFS can we guarantee a least cost solution? Justify your answer and show the results of the different strategies by using the code developed during the lab. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d51c40",
   "metadata": {},
   "source": [
    "Yes, given a unitary cost per displacement and BFS+graph search, we always get the optimal solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76c11d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution (BFS graph-search): [(1, 1), (1, 0), (2, 0), (3, 0), (3, 1)]\n",
      "NÂ° of nodes explored: 39\n",
      "Max nÂ° of nodes in memory: 11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def BFS_GraphSearch(problem):\n",
    "    \"\"\"\n",
    "    Graph Search BFS\n",
    "    \n",
    "    Args:\n",
    "        problem: OpenAI Gym environment\n",
    "        \n",
    "    Returns:\n",
    "        (path, time_cost, space_cost): solution as a path and stats.\n",
    "    \"\"\"\n",
    "    \n",
    "    node = Node(problem.startstate, None)\n",
    "    time_cost = 1\n",
    "    space_cost = 1\n",
    "    \n",
    "    if problem.goalstate == node.state: \n",
    "        return build_path(node), time_cost, space_cost\n",
    "    \n",
    "    frontier = NodeQueue()\n",
    "    frontier.add(node)\n",
    "    explored = set()\n",
    "    \n",
    "    while not frontier.is_empty():\n",
    "    \n",
    "        node = frontier.remove() # Retrieve the first node of the fringe\n",
    "        explored.add(node.state)\n",
    "        \n",
    "        for action in range(problem.action_space.n): # Look around\n",
    "            \n",
    "            child = Node(problem.sample(node.state, action), node) # Child node\n",
    "            time_cost += 1\n",
    "            \n",
    "            if child.state not in frontier and child.state not in explored: # Add if not in list and not explored\n",
    "                if problem.goalstate == child.state: # Goal state check\n",
    "                    return build_path(child), time_cost, space_cost\n",
    "                frontier.add(child)\n",
    "                \n",
    "        space_cost = max(space_cost, len(frontier) + len(explored)) \n",
    "        \n",
    "    return None, time_cost, space_cost\n",
    "\n",
    "\n",
    "solution_gs, time_gs, memory_gs = BFS_GraphSearch(env)\n",
    "print(\"Solution (BFS graph-search): {}\".format(solution_2_string(solution_gs, env)))\n",
    "print(\"NÂ° of nodes explored: {}\".format(time_gs))\n",
    "print(\"Max nÂ° of nodes in memory: {}\\n\".format(memory_gs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0266db",
   "metadata": {},
   "source": [
    "### Using IDS+graph search in this instance we obtain the least cost solution. However, we can not guarantee this in general. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1bfe7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution (IDS graph-search): [(1, 1), (1, 0), (2, 0), (3, 0), (3, 1)]\n",
      "NÂ° of nodes explored: 41\n",
      "Max nÂ° of nodes in memory: 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#example not optmality IDS+GRAPH SEARCH\n",
    "\n",
    "\n",
    "def DLS(problem, limit, RDLS_Function):\n",
    "    \"\"\"\n",
    "    DLS\n",
    "    \n",
    "    Args:\n",
    "        problem: OpenAI Gym environment\n",
    "        limit: depth limit for the exploration, negative number means 'no limit'\n",
    "        \n",
    "    Returns:\n",
    "        (path, time_cost, space_cost): solution as a path and stats.\n",
    "    \"\"\"\n",
    "        \n",
    "    node = Node(problem.startstate, None)\n",
    "    return RDLS_Function(node, problem, limit, set())\n",
    "\n",
    "def Recursive_DLS_GraphSearch(node, problem, limit, explored):\n",
    "    \"\"\"\n",
    "    Recursive DLS\n",
    "    \n",
    "    Args:\n",
    "        node: node to explore\n",
    "        problem: OpenAI Gym environment\n",
    "        limit: depth limit for the exploration, negative number means 'no limit'\n",
    "        explored: completely explored nodes\n",
    "        \n",
    "    Returns:\n",
    "        (path, time_cost, space_cost): solution as a path and stats.\n",
    "    \"\"\"\n",
    "    \n",
    "    if problem.goalstate == node.state: # Goal state check\n",
    "        return build_path(node), 1, node.depthcost\n",
    "    elif limit == 0: # Limit budget check for cutoff\n",
    "        return \"cut_off\", 1, node.depthcost\n",
    "    \n",
    "    explored.add(node.state)\n",
    "    space_cost = node.depthcost\n",
    "    time_cost = 1 \n",
    "    \n",
    "    cut_off_occurred = False\n",
    "    for action in range(problem.action_space.n): # Look around\n",
    "        child = Node(problem.sample(node.state, action), node, node.pathcost+1) # Child node\n",
    "        \n",
    "        if child.state in explored: # Add if not explored\n",
    "            continue\n",
    "            \n",
    "        result = Recursive_DLS_GraphSearch(child, problem, limit-1, explored) # Recursive call\n",
    "        time_cost += result[1]\n",
    "        space_cost = max(space_cost, result[2])    \n",
    "        \n",
    "        if result[0] == \"cut_off\":\n",
    "            cut_off_occurred = True\n",
    "        elif result[0] != \"failure\": # Solution found\n",
    "            return result[0], time_cost, space_cost\n",
    "        \n",
    "    if cut_off_occurred: # Solution not found but cutoff occurred\n",
    "        return \"cut_off\", time_cost, space_cost\n",
    "    else: # No solution and no cutoff: failure\n",
    "        return \"failure\", time_cost, space_cost\n",
    "\n",
    "\n",
    "def IDS(env, DLS_Function):\n",
    "    \"\"\"\n",
    "    Iteartive_DLS DLS\n",
    "    \n",
    "    Args:\n",
    "        problem: OpenAI Gym environment\n",
    "        \n",
    "    Returns:\n",
    "        (path, time_cost, space_cost): solution as a path and stats.\n",
    "    \"\"\"\n",
    "    total_time_cost = 0\n",
    "    total_space_cost = 1\n",
    "    \n",
    "    for i in zero_to_infinity():\n",
    "        solution_dls, time_dls, memory_dls = DLS(env, i, DLS_Function)\n",
    "        total_time_cost += time_dls\n",
    "        total_space_cost = max(total_space_cost, memory_dls)\n",
    "        if isinstance(solution_dls, type(tuple)):\n",
    "            break\n",
    "            \n",
    "    return solution_dls, total_time_cost, total_space_cost, i\n",
    "\n",
    "\n",
    "solution_gs, time_gs, memory_gs, iterations_gs = IDS(env, Recursive_DLS_GraphSearch)\n",
    "print(\"Solution (IDS graph-search): {}\".format(solution_2_string(solution_gs, env)))\n",
    "print(\"NÂ° of nodes explored: {}\".format(time_gs))\n",
    "print(\"Max nÂ° of nodes in memory: {}\\n\".format(memory_gs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bc2c64",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de20f428",
   "metadata": {},
   "source": [
    "Consider the environment displayed in Figure below, where states $(0, 3)$ and $(1, 3)$ are terminal states with reward $+1$ and $â1$ respectively. The agent can move in the four directions. The transition model states that for every state and action the agent has $0.8$ chances of moving in the chosen direction and $0.1$ chances to move in the othogonal directions.The reward model states that for every state, action and successor state the agent pays $â0.04$. Assume that the dicount factor is set to $0.9$. Answer the following questions: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b78047",
   "metadata": {},
   "source": [
    "<img src=\"images/ex3.1.png\" alt=\"ex3\" style=\"zoom: 40%;\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf24b52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['S' 'L' 'L' 'G']\n",
      " ['L' 'W' 'L' 'P']\n",
      " ['L' 'L' 'L' 'L']]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('VeryBadLavaFloor-v0')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac451e5",
   "metadata": {},
   "source": [
    "### 3.1) Use one of the methods developed in the lab to compute the value function for the left diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "490b94e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(environment, maxiters=500, discount=0.9, max_error=1e-3):\n",
    "    \"\"\"\n",
    "    Performs the value iteration algorithm for a specific environment\n",
    "    \n",
    "    Args:\n",
    "        environment: OpenAI Gym environment\n",
    "        maxiters: timeout for the iterations\n",
    "        discount: gamma value, the discount factor for the Bellman equation\n",
    "        max_error: the maximum error allowd in the utility of any state\n",
    "        \n",
    "    Returns:\n",
    "        policy: 1-d dimensional array of action identifiers where index `i` corresponds to state id `i`\n",
    "    \"\"\"\n",
    "    \n",
    "    U_1 = [0 for _ in range(environment.observation_space.n)] # vector of utilities for states S\n",
    "    delta = 0 # maximum change in the utility o any state in an iteration\n",
    "    \n",
    "    while True:\n",
    "        maxiters -= 1\n",
    "        U = U_1.copy()\n",
    "        delta = 0\n",
    "        for state in range(environment.observation_space.n):\n",
    "            \n",
    "            max_array = [0 for _ in range(environment.action_space.n)] # for each possible action\n",
    "            for action in range(environment.action_space.n):\n",
    "                for next_state in range(environment.observation_space.n):\n",
    "                    max_array[action] += env.T[state, action, next_state] * U[next_state]\n",
    "                    \n",
    "            if env.grid[state] == \"P\" or env.grid[state] == \"G\":\n",
    "                U_1[state] = environment.RS[state]\n",
    "            else:           \n",
    "                U_1[state] = environment.RS[state] + discount * max(max_array) # Bellman Update \n",
    "                \n",
    "            if abs(U_1[state] - U[state]) > delta: \n",
    "                delta = abs(U_1[state] - U[state])         \n",
    "                \n",
    "        if maxiters <= 0 or delta < (max_error*(1-discount)/discount):\n",
    "            break  \n",
    "  \n",
    "    return values_to_policy(np.asarray(U), environment), np.asarray(U) # automatically convert the value matrix U to a policy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da933217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.50939438  0.64958568  0.79536209  1.          0.39844322  0.\n",
      "  0.48644002 -1.          0.29628832  0.253867    0.34475423  0.12987275]\n",
      "[['R' 'R' 'R' 'L']\n",
      " ['U' 'L' 'U' 'L']\n",
      " ['U' 'R' 'U' 'L']]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('VeryBadLavaFloor-v0')\n",
    "policy, values = value_iteration(env)\n",
    "policy_render = np.vectorize(env.actions.get)(policy.reshape(env.rows, env.cols))\n",
    "print(values)\n",
    "print(policy_render)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8216357",
   "metadata": {},
   "source": [
    "### Results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66795ad",
   "metadata": {},
   "source": [
    "<img src=\"images/ex3.png\" alt=\"ex3\" style=\"zoom: 40%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b180254a",
   "metadata": {},
   "source": [
    "### 3.2) Consider the right diagram and focus on states $(2, 1)$. State whether the action reported in the right diagram (the blue one) represents the optimal action for that state. Motivate your answer with the code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ebad340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21902365 0.25391911 0.20047807 0.20047807]\n",
      "The correct action to perform should be: R\n"
     ]
    }
   ],
   "source": [
    "actions = {0: \"L\", 1: \"R\", 2: \"U\", 3: \"D\"}\n",
    "\n",
    "value_function = [0.50939438, 0.64958568, 0.79536209, 1, 0.39844322, 0, 0.48644002, -1, 0.29628832,  0.253867, 0.34475423, 0.12987275]\n",
    "id_start_state = 9\n",
    "gamma = 0.9\n",
    "\n",
    "values_ex = [0, 0, 0, 0]\n",
    "for action in range(env.action_space.n):\n",
    "    for next_state in range(env.observation_space.n):\n",
    "        values_ex[action] += env.T[id_start_state, action, next_state] * (env.RS[id_start_state]+ gamma*value_function[next_state])\n",
    "    \n",
    "print(np.asarray(values_ex))\n",
    "print(f'The correct action to perform should be: {actions[np.argmax(values_ex)]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611fa5fc",
   "metadata": {},
   "source": [
    "### 3.3) Compute the probability of ending in state (1, 3) if we execute the sequence of actons < Up, Up > from state (2, 2). Motivate your answer reporting the code and the solution printed. The following image shows a diagram to guide the solution process as a hint for you:\n",
    "\n",
    "\n",
    "<img src=\"images/example.png\" alt=\"ex3\" style=\"zoom: 30%;\" />\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2369c26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2) --> (1, 2) --> (1, 3)\n",
      "probs: 0.8 --> 0.1\n",
      "================\n",
      "(2, 2) --> (2, 3) --> (1, 3)\n",
      "probs: 0.1 --> 0.8\n",
      "================\n",
      "\n",
      "Probability:  0.16\n"
     ]
    }
   ],
   "source": [
    "id_start_state = 10\n",
    "state = id_start_state\n",
    "actions = {0: \"L\", 1: \"R\", 2: \"U\", 3: \"D\"}\n",
    "\n",
    "prob = 0\n",
    "action = 2\n",
    "probs_fin = 0\n",
    "\n",
    "for next_state in range(env.observation_space.n):\n",
    "    if env.T[state, action, next_state] == 0:\n",
    "        continue\n",
    "\n",
    "    probs = env.T[state, action, next_state]\n",
    "    \n",
    " \n",
    "    for second_next_state in range(env.observation_space.n):\n",
    "        if env.T[next_state, action, second_next_state] == 0:\n",
    "            continue\n",
    "\n",
    "        if env.grid[second_next_state] == \"P\":\n",
    "    \n",
    "            probs *= env.T[next_state, action, second_next_state]\n",
    "\n",
    "            print(f'{env.state_to_pos(state)} --> {env.state_to_pos(next_state)} --> {env.state_to_pos(second_next_state)}')\n",
    "            print(f'probs: {env.T[state, action, next_state]} --> {env.T[next_state, action, second_next_state]}')\n",
    "\n",
    "            # your code here\n",
    "            probs_fin += probs\n",
    "\n",
    "            print('================')\n",
    "            break\n",
    "    \n",
    "print()\n",
    "print('Probability: ', round(probs_fin,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455c0594",
   "metadata": {},
   "source": [
    "### 3.4) Consider the following environment where states $(0, 3)$ and $(1, 3)$ are terminal states with reward $+1$ and $â1$ respectively. Assume the transition model is the same one defined above and that the discounted factor is $0.9$ as above. However, now the agent pays $â0.4$ instead of $â0.04$ for every action, state and successor state.  Compute the new value function and the optimal policy for this new environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89237f75",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"images/env_3.4.png\" alt=\"ex3\" style=\"zoom: 40%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "164ca1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.71058163 -0.20355824  0.32372581  1.         -1.11176358  0.\n",
      " -0.28232707 -1.         -1.43861265 -1.20665118 -0.81864385 -1.18620466]\n",
      "[['R' 'R' 'R' 'L']\n",
      " ['U' 'L' 'U' 'L']\n",
      " ['U' 'R' 'U' 'L']]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('NiceLavaFloor-v0')\n",
    "policy, values = value_iteration(env, discount=0.9)\n",
    "policy_render = np.vectorize(env.actions.get)(policy.reshape(env.rows, env.cols))\n",
    "print(values)\n",
    "print(policy_render)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e4b4e0",
   "metadata": {},
   "source": [
    "### 3.5) Modify the value iteration parameters so that the policy allows the agent to reach the goal only starting from states $(0,1)$, $(0,2)$ and $(1,2)$, as reported in the image below . Motivate your answer.\n",
    "\n",
    "\n",
    "#### hint: given the negative reward the agent should care about the immediate reward and not the long-term reward.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfd75f5",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"images/result_ex3.5.png\" alt=\"ex3\" style=\"zoom: 40%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6d7eed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.44  -0.44  -0.328  1.    -0.44   0.    -0.44  -1.    -0.44  -0.44\n",
      " -0.44  -0.44 ]\n",
      "[['L' 'R' 'R' 'L']\n",
      " ['L' 'L' 'U' 'L']\n",
      " ['L' 'L' 'L' 'D']]\n"
     ]
    }
   ],
   "source": [
    "# use value iteration to check \n",
    "#sol = tuning discout to 0.1\n",
    "env = gym.make('NiceLavaFloor-v0')\n",
    "policy, values = value_iteration(env, discount=0.1)\n",
    "policy_render = np.vectorize(env.actions.get)(policy.reshape(env.rows, env.cols))\n",
    "print(values)\n",
    "print(policy_render)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff318dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "703075dc036e8ebc3a027aec30cd295176a007527fa40434b7705e84e779ac0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
